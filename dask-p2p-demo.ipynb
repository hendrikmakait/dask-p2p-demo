{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d3b3802-f690-4348-9f5e-04fa52acc549",
   "metadata": {},
   "source": [
    "# Shuffling large data at constant memory in Dask\n",
    "\n",
    "**A showcase for P2P shuffling/rechunking at Dask Demo Day 2023-03-16**\n",
    "\n",
    "To learn more, check out our blog post at [https://blog.coiled.io/blog/shuffling-large-data-at-constant-memory/](https://blog.coiled.io/blog/shuffling-large-data-at-constant-memory/)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29d3b23-57b1-424e-b575-7c5b8377cf44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import coiled\n",
    "import dask\n",
    "from distributed import Client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11838d56-4bd7-4126-8b1c-9a84e68c8723",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Define utilities\n",
    "\n",
    "Sources: \n",
    "* [https://github.com/coiled/coiled-runtime/blob/c8540241e1c2b19d9348e57e12ac62c689463100/tests/utils_test.py](https://github.com/coiled/coiled-runtime/blob/c8540241e1c2b19d9348e57e12ac62c689463100/tests/utils_test.py)\n",
    "* [https://github.com/coiled/coiled-runtime/blob/c8540241e1c2b19d9348e57e12ac62c689463100/tests/benchmarks/test_dataframe.py](https://github.com/coiled/coiled-runtime/blob/c8540241e1c2b19d9348e57e12ac62c689463100/tests/benchmarks/test_dataframe.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e30aa3-5bb2-45d8-ac24-14c0b2cfa5b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import distributed\n",
    "import pandas as pd\n",
    "from dask.datasets import timeseries\n",
    "from dask.sizeof import sizeof\n",
    "from dask.utils import format_bytes, parse_bytes\n",
    "\n",
    "def cluster_memory(client: distributed.Client) -> int:\n",
    "    \"\"\"Total memory available on the cluster, in bytes\"\"\"\n",
    "    return int(\n",
    "        sum(w[\"memory_limit\"] for w in client.scheduler_info()[\"workers\"].values())\n",
    "    )\n",
    "\n",
    "\n",
    "def timeseries_of_size(\n",
    "    target_nbytes: int | str,\n",
    "    *,\n",
    "    start=\"2000-01-01\",\n",
    "    freq=\"1s\",\n",
    "    partition_freq=\"1d\",\n",
    "    dtypes={\"name\": str, \"id\": int, \"x\": float, \"y\": float},\n",
    "    seed=None,\n",
    "    **kwargs,\n",
    ") -> dd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate a `dask.demo.timeseries` of a target total size.\n",
    "\n",
    "    Same arguments as `dask.demo.timeseries`, but instead of specifying an ``end`` date,\n",
    "    you specify ``target_nbytes``. The number of partitions is set as necessary to reach\n",
    "    approximately that total dataset size. Note that you control the partition size via\n",
    "    ``freq``, ``partition_freq``, and ``dtypes``.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> timeseries_of_size(\n",
    "    ...     \"1mb\", freq=\"1s\", partition_freq=\"100s\", dtypes={\"x\": float}\n",
    "    ... ).npartitions\n",
    "    278\n",
    "    >>> timeseries_of_size(\n",
    "    ...     \"1mb\", freq=\"1s\", partition_freq=\"100s\", dtypes={i: float for i in range(10)}\n",
    "    ... ).npartitions\n",
    "    93\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    The ``target_nbytes`` refers to the amount of RAM the dask DataFrame would use up\n",
    "    across all workers, as many pandas partitions.\n",
    "\n",
    "    This is typically larger than ``df.compute()`` would be as a single pandas\n",
    "    DataFrame. Especially with many partions, there can be significant overhead to\n",
    "    storing all the individual pandas objects.\n",
    "\n",
    "    Additionally, ``target_nbytes`` certainly does not correspond to the size\n",
    "    the dataset would take up on disk (as parquet, csv, etc.).\n",
    "    \"\"\"\n",
    "    if isinstance(target_nbytes, str):\n",
    "        target_nbytes = parse_bytes(target_nbytes)\n",
    "\n",
    "    start_dt = pd.to_datetime(start)\n",
    "    partition_freq_dt = pd.to_timedelta(partition_freq)\n",
    "    example_part = timeseries(\n",
    "        start=start,\n",
    "        end=start_dt + partition_freq_dt,\n",
    "        freq=freq,\n",
    "        partition_freq=partition_freq,\n",
    "        dtypes=dtypes,\n",
    "        seed=seed,\n",
    "        **kwargs,\n",
    "    )\n",
    "    p = example_part.compute(scheduler=\"threads\")\n",
    "    partition_size = sizeof(p)\n",
    "    npartitions = round(target_nbytes / partition_size)\n",
    "    assert npartitions > 0, (\n",
    "        f\"Partition size of {format_bytes(partition_size)} > \"\n",
    "        f\"target size {format_bytes(target_nbytes)}\"\n",
    "    )\n",
    "\n",
    "    ts = timeseries(\n",
    "        start=start,\n",
    "        end=start_dt + partition_freq_dt * npartitions,\n",
    "        freq=freq,\n",
    "        partition_freq=partition_freq,\n",
    "        dtypes=dtypes,\n",
    "        seed=seed,\n",
    "        **kwargs,\n",
    "    )\n",
    "    assert ts.npartitions == npartitions\n",
    "    return ts\n",
    "\n",
    "def print_dataframe_info(df):\n",
    "    p = df.partitions[0].compute(scheduler=\"threads\")\n",
    "    partition_size = sizeof(p)\n",
    "    total_size = partition_size * df.npartitions\n",
    "    print(\n",
    "        f\"~{len(p) * df.npartitions:,} rows x {len(df.columns)} columns, \"\n",
    "        f\"{format_bytes(total_size)} total, \"\n",
    "        f\"{df.npartitions:,} {format_bytes(partition_size)} partitions\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96c75fc-2355-4857-9371-ef6989229fce",
   "metadata": {
    "tags": []
   },
   "source": [
    "## What is shuffling?\n",
    "\n",
    "**TL;DR:** Shuffling is used whenever we move a dataset around in an all-to-all fashion, such as occurs in sorting, dataframe joins, or array rechunking.\n",
    "\n",
    "![](https://assets-global.website-files.com/63192998e5cab906c1b55f6e/633f7b5df9c63728c2ce7ac6_image-3-700x340.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c6559d-bd0d-47ab-b58b-f07ef75f40e1",
   "metadata": {},
   "source": [
    "## Problem: Task-based shuffling scales poorly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a155a896-48d5-4e45-b1f7-c1939e8a5e01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from coiled import Cluster\n",
    "\n",
    "tasks_cluster = Cluster(\n",
    "    name=\"dask-p2p-demo-tasks\",\n",
    "    n_workers=10,\n",
    "    shutdown_on_close=False,\n",
    "    wait_for_workers=True,\n",
    "    worker_vm_types=\"m6i.large\", \n",
    "    scheduler_vm_types=[\"m6i.large\"],\n",
    "    scheduler_options={\"idle_timeout\": \"1 hours\"}\n",
    ")\n",
    "tasks_client = Client(tasks_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89d3d45-8174-48d7-8d36-c2270f1f2b6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory = cluster_memory(tasks_client)\n",
    "format_bytes(memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66907e64-7acc-4687-9785-17c1c0789844",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "\n",
    "df = timeseries_of_size(\n",
    "    memory,\n",
    "    start=\"2020-01-01\",\n",
    "    freq=\"600ms\",\n",
    "    partition_freq=\"24h\",\n",
    "    dtypes={str(i): float for i in range(100)},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634ef27a-fd8e-46b9-8403-cf9cdf453b28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print_dataframe_info(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b1e0f2-45b2-4380-807b-b68031055abc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with dask.config.set({\"dataframe.shuffle.method\": \"tasks\"}):\n",
    "    shuffled = df.shuffle(\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5942fe-e217-4ee7-bccc-83fce811e09c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "final = shuffled.size\n",
    "print(tasks_client.dashboard_link)\n",
    "f1 = tasks_client.compute(final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104f2d25-7852-42da-8122-ca943883f0bf",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Solution: P2P shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf0a843-522e-4977-945c-9b547103cc69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from coiled import Cluster\n",
    "\n",
    "p2p_cluster = Cluster(\n",
    "    name=\"dask-p2p-demo-p2p\",\n",
    "    n_workers=10,\n",
    "    shutdown_on_close=False,\n",
    "    wait_for_workers=True,\n",
    "    worker_vm_types=\"m6i.large\", \n",
    "    scheduler_vm_types=[\"m6i.large\"],\n",
    "    scheduler_options={\"idle_timeout\": \"1 hours\"}\n",
    ")\n",
    "p2p_client = Client(p2p_cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cfeae76-99e7-4a63-b5f7-53ab100d3558",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Restart task-based cluster\n",
    "tasks_client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf2d771-2fa4-428f-9a63-12cda4ee8dcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Task-based dashboard: {tasks_client.dashboard_link}\")\n",
    "with dask.config.set({\"dataframe.shuffle.method\": \"tasks\"}):\n",
    "    shuffled = df.shuffle(\"0\")\n",
    "f1 = tasks_client.compute(shuffled.size)\n",
    "\n",
    "print(f\"P2P dashboard: {p2p_client.dashboard_link}\")\n",
    "shuffled = df.shuffle(\"0\")\n",
    "f2 = p2p_client.compute(shuffled.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c22d055-7f0c-4f79-a2fd-8da7ddc9e7ca",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preview: P2P rechunking for arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6aa45a-aa77-4a48-a70e-39f6ccc8fc1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import dask.array as da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a710ee-158a-4d4a-83c9-ed426b619440",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Restart clusters\n",
    "tasks_client.restart()\n",
    "p2p_client.restart()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01644f4-4447-43c2-8337-66f6da5f24e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "shape = (nt, ny, nx) = (2500, 1800, 3600)\n",
    "chunks = (1, ny, nx)\n",
    "arr = da.random.random(shape, chunks=chunks)\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72dbfe99-1c70-4f43-b85a-cb0339b1d754",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Task-based dashboard: {tasks_client.dashboard_link}\")\n",
    "rechunked = arr.rechunk((-1, 90, 36))    \n",
    "f1 = tasks_client.compute(rechunked.sum())\n",
    "\n",
    "print(f\"P2P dashboard: {p2p_client.dashboard_link}\")\n",
    "with dask.config.set({\"optimization.fuse.active\": False, \"array.rechunk.method\": \"p2p\"}):\n",
    "    rechunked = arr.rechunk((-1, 90, 36))\n",
    "    f2 = p2p_client.compute(rechunked.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc180d9-ffd6-438b-8dde-4948d2e28e49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "await f2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
